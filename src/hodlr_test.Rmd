---
title: "KRLS HODLR benchmarking"
author: "Alex Kindel"
date: "4 July 2017"
output: html_document
---

```{r setup}
require(KRLS)
require(bigKRLS)
require(dplyr)
require(magrittr)
require(Rcpp)
require(rbenchmark)
require(inline)
```

## Test data

### Harff (2003) genocide data

```{r harff}
# Load dataset from Harff (2003)
# 126 observations x 6 variables
load("/Users/AK/Code/pdqkrls/src/data/Harff2003.RData")
harff_y <- as.matrix(d$Genocideonset)
harff_X <- as.matrix(d[,-1])
```

### Hainmueller & Hazlett (2013) synthetic interaction data

```{r 4bsim}
# Simulate complex interactions from H&H (2013), fig. 4B
# Adapted from H&H (2013) replication materials
# Using 3k observations to test, original is 10k
n <- 3000

# Generate X
fig4b_X <- replicate(10, rbinom(n, 1, .5))
fig4b_X[,1:2] <- replicate(2, rbinom(n, 1, .25))
fig4b_X[,3:4] <- replicate(2, rbinom(n, 1, .75))

# Y generated by a complex interaction among X variables with random perturbations
ytruefn <- function(X) { X[,1]*X[,2]-2*X[,3]*X[,4]+X[,5]*X[,6]*3*X[,7]-X[,1]*X[,8]+2*X[,8]*X[,9]*X[,10]+X[,10] }
ytrue <- ytruefn(fig4b_X)
fig4b_y <- as.matrix(ytrue + rnorm(n, mean=0, sd=0.5))
```

## Benchmark results

### KRLS

First, we compute the full KRLS estimate over the Harff and synthetic datasets:

```{r krls_vanilla, eval=FALSE}
# Harff data takes around 0.2s
system.time(
  krls_vanilla_harff <- krls(X=harff_X, y=harff_y)
)

# Synth data takes around 38 minutes (!)
system.time(
  krls_vanilla_fig4b <- krls(X=fig4b_X, y=fig4b_y)
)
```

### bigKRLS

```{r bigkrls_harff}
# Throws an error when attempting to compute fitted values
system.time(
  bigkrls_harff <- bigKRLS(X=harff_X, y=harff_y)
)

# Throws an error when attempting to compute lambda
system.time(
  bigkrls_fig4b <- bigKRLS(X=fig4b_X, y=fig4b_y)
)
```

### HODLR



## Benchmark results

### Baseline KRLS

TODO: Check with a 5000x10 rectangular dataset

```{r krls_subset, eval=FALSE}
# Benchmark by size of subset N
# This takes a long time, too
krls_test <- function(data, n) {
  # Subset data
  data_subset <- sample_n(data, n)
  data_subset %>%
  dplyr::select(Number.of.Registered.Voters,
                Number.of.Ballots.Received,
                Number.of.Ballots.issued.to.Early.Voters,
                Ballots.issued.in.Polling.Place.on.Election.Day,
                Ballots.issued.outside.of.the.polling.station.on.election.day,
                Number.of.Cancelled.Ballots,
                Number.of.Ballots.in.Mobile.Ballot.Boxes,
                Number.of.Ballots.in.Stationary.Boxes,
                Number.of.Valid.Ballots,
                Received.Absentee.Ballots) ->
  datX
  data_subset %>%
    dplyr::select(Putin) ->
    datY
  
  # Compute and yield KRLS estimates
  return(krls(X=datX, y=datY, whichkernel="gaussian", print.level=0))
}
benchmark(krls_test(russia, 100),
          krls_test(russia, 200),
          krls_test(russia, 500),
          krls_test(russia, 1000))
```

### Small dataset KRLS

TODO: This is just for testing; full diagnostics will use benchmarking as above

```{r krls_small}
# Takes around 40s
# This is what we're testing more directly below
system.time(
  krls_small <- krls(X=rusX_small, y=rusY_small,
                     whichkernel = "gaussian",
                     print.level = 2)
)
```

### bigKRLS

TODO

```{r bigKRLS, eval=FALSE}
# 5000 observations nukes R (system.time contains a call to save, I think?)
# 1000 observations fails (missing parameter internally)
system.time(
  krls_big <- bigKRLS(y=as.matrix(rusY_small),
                      X=as.matrix(rusX_small))
)
```

## Matrix inversion step

```{r preprocess}
# Scale X, Y (as in krls.R)
X.init <- rusX_small
X.init.sd <- apply(X.init, 2, sd)
X <- scale(rusX_small, center=TRUE, scale=X.init.sd)

y.init <- rusY_small
y.init.sd <- apply(y.init,2,sd)
y.init.mean <- mean(y.init)
y <- scale(rusY_small,center=y.init.mean,scale=y.init.sd)

# Compute Gaussian kernel matrix over scaled subset data
# Bandwidth defaults to dimension of X per H&H14
K <- gausskernel(X, sigma=ncol(X))

# LOOE of lambda is derived from above KRLS call
lambda <- 0.79504
```

```{r krls_inverse}
# Invert G=K+lI using eigendecomposition (loosely following solveforc.R)
krlsinvert <- function(K, lambda) {
  # Function to multiply square and diagonal matrix (as in multdiag.R)
  multdiag <- function(X,d){	
  	R=matrix(NA,nrow=dim(X)[1],ncol=dim(X)[2])		
  	for (i in 1:dim(X)[2]){
  		R[,i]=X[,i]*d[i]	
  	}
  	return(R)
  } 
  
  # Compute regularized matrix inverse
  # Don't truncate eigenvalues for now
  Eigenobject <- eigen(K, symmetric=TRUE)
  Ginv <- tcrossprod(multdiag(X=Eigenobject$vectors,
                              d=1/(Eigenobject$values+lambda)),
                     Eigenobject$vectors)
  
  return(Ginv)
}
time_krls <- system.time(Ginv_krls <- krlsinvert(K, lambda))
time_krls
```

With n = 5000 observations and d = 10 variables, HODLR appears to take ~0.4s compared to ~300s for eigendecomposition in R.

```{r hodlr_inverse}
# Invert G=K+lI by throwing down to HODLR
hodlr <- ""  # TODO: Implement this
hodlrinvert <- function(K, N, lambda) {
  iv <- cxxfunction(sig = signature(K="numeric", N="integer", lambda="numeric"),
                    hodlr, plugin = "Rcpp")
  return(iv(K, N, lambda))
}
time_hodlr <- system.time(Ginv_hodlr <- hodlrinvert(K, nrow(K), lambda))
```

## Accuracy and performance

Over standard dataset, compare:  
- Model predictions
- Empirical (wall-clock) time performance
- Profiles

TODO

## Estimating shrinkage parameter

- KRLS and bigKRLS approximate LOOE with golden selection search
- This requires inverting K once per selection of lambda, i.e. time complexity scales cubically in # guesses
- In general, doesn't seem possible to compute updates to the inverted matrix more efficiently
  - Check k-fold CV performance; would expect tradeoff between variance of lambda estimate and speed
- Other ways of quickly approximating lambda?
  - Truncate search?

## Sorting in HODLR

- What makes the preprocessing step expensive?
- Would sorting be affected by the regularization term on the diagonal? (intuitively, no)
